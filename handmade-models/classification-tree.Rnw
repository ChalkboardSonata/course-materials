\documentclass{article}
\usepackage{graphicx, color, hyperref, fancyhdr}

%\input{../brayTeachingStyle}

\usepackage[top=.5in, bottom=.5in, left=1.1in, right=1.1in]{geometry}
\thispagestyle{empty}
\begin{document}

\begin{center}
\textsc{Math 243: Statistical Learning} \\
\noindent\rule{12cm}{0.4pt}
\end{center}


\begin{enumerate}
\setlength\itemsep{4em}

\item The plot below represents the predictor space (on $X_1$ and $X_2$) with a training data set plotted and the class of their response variable indicated by the color.

\begin{minipage}{.5\linewidth}

\vspace{3mm}

<<echo = FALSE, fig.height=4.5, fig.width = 4.8>>=
set.seed(75)
n <- 16
x1 <- runif(n)
x2 <- runif(n)
group <- as.factor(sample(1:3, n, replace = TRUE))
levels(group) <- c("circle", "triangle", "square")
df <- data.frame(x1, x2, group)
df[1, 2] <- .765 # tweaks to make a more interesting configuration
df[9, 1] <- .741
df <- df[-7, ]

library(ggplot2)
ggplot(df, aes(x = x1, y = x2, col = group, shape = group)) +
  geom_point(size = 4) +
  scale_x_continuous(expand = c(0, 0) , limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_color_discrete(guide = FALSE) +
  scale_shape_discrete(guide = FALSE) +
  labs(x = expression(X[1]),
       y = expression(X[2])) +
  theme_bw()
@

\end{minipage}
\begin{minipage}{.5\linewidth}

\begin{enumerate}
\setlength\itemsep{3em}
\item If we consider this a classification tree without any splits yet (i.e. only one region), what would be the prediction for \emph{every} new observation?
\item What is the (training) misclassification rate?
\item What is the GINI index?
\item What is the cross-entropy?
\end{enumerate}

\end{minipage}

\item Add a straight line, parallel to one of the axes, that splits the predictor space into two regions. Choose the split in a way that you think will lead to the best overall improvement in the metrics above. Label the new regions $\textrm{R}_1$ and $\textrm{R}_2$ and calculate the metrics for each.

\vspace{5mm}

\begin{minipage}{.5\linewidth}
\begin{center}
$\textrm{R}_1$
\end{center}
\begin{enumerate}
\setlength\itemsep{3em}
\item What is the predicted class?
\item What is the misclassification rate?
\item What is the GINI index?
\item What is the cross-entropy?
\end{enumerate}
\end{minipage}
\begin{minipage}{.5\linewidth}
\begin{center}
$\textrm{R}_2$
\end{center}
\begin{enumerate}
\setlength\itemsep{3em}
\item What is the predicted class?
\item What is the misclassification rate?
\item What is the GINI index?
\item What is the cross-entropy?
\end{enumerate}

\end{minipage}
\item To decide if the split in Q2 was optimal, we need to evaluate how much the metrics in Q1 have improved. This requires combining the metrics across $\textrm{R}_1$ and $\textrm{R}_2$ in Q2. Please do so in a sensible way so that you can answer: what was the overall decrease each metric going from one region/node to two?

Misclassification: \hspace{30mm} GINI: \hspace{30mm} Cross-entropy:

\vspace{5mm}

\item On the back of this page, please draw the (very simple) tree corresponding to your partition.

\end{enumerate}

\end{document}